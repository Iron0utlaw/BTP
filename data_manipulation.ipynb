{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = \"humarin-chatgpt-paraphrases.txt\"\n",
    "output_file = \"output.csv\"\n",
    "\n",
    "# Open the input and output files\n",
    "with open(input_file, 'r') as infile, open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "    # Initialize CSV writer\n",
    "    csv_writer = csv.writer(outfile)\n",
    "    \n",
    "    # Write the header row\n",
    "    csv_writer.writerow([\"Prompt\", \"Completion\"])\n",
    "    \n",
    "    # Process each line in the input file\n",
    "    for line in infile:\n",
    "        try:\n",
    "            # Parse the JSON object\n",
    "            data = json.loads(line.strip())\n",
    "            # Write the values to the CSV\n",
    "            csv_writer.writerow([data[\"prompt\"], data[\"completion\"]])\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Skipping invalid JSON line: {line}\")\n",
    "        except KeyError:\n",
    "            print(f\"Skipping line with missing keys: {line}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'group' to divide the rows into blocks of 5\n",
    "df['group'] = np.floor(df.index / 5)\n",
    "\n",
    "# Randomly select one row from each group\n",
    "filtered_df = df.groupby('group').apply(lambda x: x.sample(1)).reset_index(drop=True)\n",
    "\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_df = filtered_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "shuffled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the shuffled completions do not match the original prompt's row\n",
    "# Shift the shuffled dataframe to avoid matching the same row\n",
    "shuffled_df['Completion'] = shuffled_df['Completion'].shift(-1)\n",
    "\n",
    "# Drop the last row as it will have a NaN completion after the shift\n",
    "shuffled_df = shuffled_df.dropna()\n",
    "\n",
    "# Create a new DataFrame with 'prompt' and 'completion' columns\n",
    "new_df = pd.DataFrame({\n",
    "    'Prompt': filtered_df['Prompt'],\n",
    "    'Completion': shuffled_df['Completion'],\n",
    "    'label': 0\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['label'] = 1\n",
    "filtered_df.drop(columns=['group'], inplace=True)\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extended = pd.concat([filtered_df, new_df], ignore_index=True)\n",
    "df_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_parquet('train-00000-of-00001.parquet')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.drop(columns=['text'], inplace=True)\n",
    "df2['claims'] = df2['claims'].apply(lambda x: str(x)\n",
    "                                    .replace('[', '')\n",
    "                                    .replace(']', '')\n",
    "                                    .replace(\"'\", \"\")\n",
    "                                    .replace('\"', '')\n",
    "                                    .replace('\\n', ''))\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_extended.groupby('label').size().plot(kind='pie', autopct='%.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'group' to divide the rows into blocks of 5\n",
    "df2['group'] = np.floor(df2.index / 2)\n",
    "\n",
    "# Randomly select one row from each group\n",
    "filtered_df2 = df2.groupby('group').apply(lambda x: x.sample(1)).reset_index(drop=True)\n",
    "shuffled_df2 = filtered_df2.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "# Ensure that the shuffled completions do not match the original prompt's row\n",
    "# Shift the shuffled dataframe to avoid matching the same row\n",
    "shuffled_df2['paraphrase'] = shuffled_df2['paraphrase'].shift(-1)\n",
    "\n",
    "# Drop the last row as it will have a NaN paraphrase after the shift\n",
    "shuffled_df2 = shuffled_df2.dropna()\n",
    "\n",
    "# Create a new DataFrame with 'prompt' and 'paraphrase' columns\n",
    "new_df2 = pd.DataFrame({\n",
    "    'Prompt': filtered_df2['claims'],\n",
    "    'Completion': shuffled_df2['paraphrase'],\n",
    "    'label': 0\n",
    "})\n",
    "\n",
    "new_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.drop(columns=['title', 'group'], inplace=True)\n",
    "df2.rename(columns={'claims' : 'Prompt','paraphrase': 'Completion'}, inplace=True)\n",
    "df2['label'] = 1\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extended2 = pd.concat([df2, new_df2], ignore_index=True)\n",
    "df_extended2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extended2.groupby('label').size().plot(kind='pie', autopct='%.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = pd.concat([df_extended, df_extended2], ignore_index=True) \n",
    "combined_data.dropna(inplace=True)\n",
    "combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.groupby('label').size().plot(kind='pie', autopct='%.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49f90a7a84e0480abf75464285bf12ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/234884 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Step 1: Load Your Dataset\n",
    "# Replace 'your_dataset.csv' with the path to your dataset file\n",
    "dataset_path = 'combined_data.csv'\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Check if the dataset has the required columns\n",
    "assert all(col in df.columns for col in ['text_1', 'text_2', 'label']), \"Dataset must have 'text_1', 'text_2', and 'label' columns.\"\n",
    "\n",
    "# Step 2: Convert Dataset into InputExample Format\n",
    "train_data = [\n",
    "    InputExample(texts=[row['text_1'], row['text_2']], label=float(row['label']))\n",
    "    for _, row in df.iterrows()\n",
    "]\n",
    "\n",
    "# Step 3: Load Pretrained Sentence Transformer\n",
    "model = SentenceTransformer('paraphrase-mpnet-base-v2')  # You can choose any suitable pre-trained model\n",
    "\n",
    "# Step 4: Prepare the DataLoader\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=16)  # Adjust batch_size as needed\n",
    "\n",
    "# Step 5: Define the Loss Function\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "# Step 6: Fine-Tune the Model\n",
    "output_path = './output/sentence-transformer-plagiarism-model'  # Path to save the fine-tuned model\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=4,  # Adjust epochs as needed\n",
    "    warmup_steps=100,  # Adjust warmup steps as needed\n",
    "    output_path=output_path\n",
    ")\n",
    "\n",
    "print(f\"Fine-tuned model saved to: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
